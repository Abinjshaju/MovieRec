{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHeHWcFHh4sM"
      },
      "source": [
        "# Movie Recommendation System\n",
        "\n",
        "This Jupyter Notebook presents a movie recommendation system using PyTorch. The dataset contains movie ratings and movie details including genres, and we utilize temporal information to enhance the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TWziuD9Ph4sR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvcMQzFckjXd",
        "outputId": "a5a326db-f7f6-4302-c15a-dd00f12bd38a"
      },
      "outputs": [],
      "source": [
        "# Data Citation:\n",
        "# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on\n",
        "# Interactive Intelligent Systems (TiiS) 5, 4: 19:1â€“19:19. <https://doi.org/10.1145/2827872>\n",
        "\n",
        "! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "68BdqtQikvVE"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('ml-latest-small.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OmO7jmrpkvZf"
      },
      "outputs": [],
      "source": [
        "# import the dataset\n",
        "movies_df = pd.read_csv('data/ml-latest-small/movies.csv')\n",
        "ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-KRTGaskvdn",
        "outputId": "24ef2ac8-61fc-4400-8c13-2c3a23b38319"
      },
      "outputs": [],
      "source": [
        "print('The dimensions of movies dataframe are:', movies_df.shape,'\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ALPeuAiBk0mM",
        "outputId": "77403ffd-5a9b-43a5-d962-e9c3968856f9"
      },
      "outputs": [],
      "source": [
        "# Take a look at movies_df\n",
        "movies_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KzPoqkWbk0oh",
        "outputId": "31d5f171-5d17-4ffb-96eb-05a1c8d17b8e"
      },
      "outputs": [],
      "source": [
        "# Take a look at ratings_df\n",
        "ratings_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TUaWt96k0q7",
        "outputId": "494a1b9f-83a1-433d-9963-73c503d870fc"
      },
      "outputs": [],
      "source": [
        "# Movie ID to movie name mapping\n",
        "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
        "n_users = len(ratings_df.userId.unique())\n",
        "n_items = len(ratings_df.movieId.unique())\n",
        "print(\"Number of unique users:\", n_users)\n",
        "print(\"Number of unique movies:\", n_items)\n",
        "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
        "print('----------')\n",
        "print(\"Number of ratings:\", len(ratings_df))\n",
        "print(\"Therefore: \", len(ratings_df) / (n_users*n_items) * 100, '% of the matrix is filled.')\n",
        "print(\"We have an incredibly sparse matrix to work with here.\")\n",
        "print(\"And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\")\n",
        "print(\"You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\")\n",
        "print(\"One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEcmO-E2h4sW"
      },
      "source": [
        "## Dataset Class\n",
        "The `MovieDataset` class extends the PyTorch `Dataset` class to handle our movie data. It includes preprocessing steps such as adding temporal features from timestamps and one-hot encoding of genres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0UaYqH1dh4sW"
      },
      "outputs": [],
      "source": [
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, ratings_df, movies_df, train=True):\n",
        "        # Add temporal features\n",
        "        self.ratings = ratings_df.copy()\n",
        "        self.ratings['datetime'] = pd.to_datetime(self.ratings['timestamp'], unit='s')\n",
        "        self.ratings['hour'] = self.ratings['datetime'].dt.hour\n",
        "        self.ratings['day_of_week'] = self.ratings['datetime'].dt.dayofweek\n",
        "\n",
        "        # Add movie features\n",
        "        movies = movies_df.copy()\n",
        "        # One-hot encode genres\n",
        "        genre_dummies = movies['genres'].str.get_dummies('|')\n",
        "        self.n_genre_features = len(genre_dummies.columns)\n",
        "        movies = pd.concat([movies, genre_dummies], axis=1)\n",
        "\n",
        "        # Merge movie features\n",
        "        self.ratings = self.ratings.merge(movies[['movieId'] + list(genre_dummies.columns)],\n",
        "                                        on='movieId', how='left')\n",
        "\n",
        "        # Create continuous IDs\n",
        "        self.userid2idx = {o:i for i,o in enumerate(self.ratings['userId'].unique())}\n",
        "        self.movieid2idx = {o:i for i,o in enumerate(self.ratings['movieId'].unique())}\n",
        "\n",
        "        # Convert IDs to indices\n",
        "        self.ratings['user_idx'] = self.ratings['userId'].map(self.userid2idx)\n",
        "        self.ratings['movie_idx'] = self.ratings['movieId'].map(self.movieid2idx)\n",
        "        # Create a reverse mapping for movie IDs\n",
        "        self.idx2movieid = {i:o for o, i in self.movieid2idx.items()}\n",
        "\n",
        "        # Split train/test\n",
        "        if train:\n",
        "            self.ratings = self.ratings.sample(frac=0.8, random_state=42)\n",
        "        else:\n",
        "            self.ratings = self.ratings.sample(frac=0.2, random_state=42)\n",
        "\n",
        "        # Scale temporal features\n",
        "        scaler = MinMaxScaler()\n",
        "        self.ratings[['hour', 'day_of_week']] = scaler.fit_transform(self.ratings[['hour', 'day_of_week']])\n",
        "\n",
        "        # Prepare features and target\n",
        "        self.features = self.ratings[['user_idx', 'movie_idx', 'hour', 'day_of_week'] +\n",
        "                                   list(genre_dummies.columns)].values\n",
        "        self.targets = self.ratings['rating'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.features[idx]), torch.FloatTensor([self.targets[idx]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE9vHDy2h4sX"
      },
      "source": [
        "## Recommender Model\n",
        "The `Recommender` class is the core of our recommendation system. We utilize embeddings to represent users and movies, and a neural network to predict ratings based on these embeddings coupled with temporal and genre features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xfx81_pLh4sX"
      },
      "outputs": [],
      "source": [
        "class Recommender(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=50, n_genres=20):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embeddings\n",
        "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
        "        self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
        "\n",
        "        # Neural network for feature processing\n",
        "        self.nn = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_factors * 2 + 2 + n_genres, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(64, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, features):\n",
        "        user_idx = features[:, 0].long()\n",
        "        movie_idx = features[:, 1].long()\n",
        "        temporal_features = features[:, 2:4]\n",
        "        genre_features = features[:, 4:]\n",
        "\n",
        "        # Get embeddings\n",
        "        user_embedding = self.user_factors(user_idx)\n",
        "        movie_embedding = self.item_factors(movie_idx)\n",
        "\n",
        "        # Concatenate all features\n",
        "        x = torch.cat([\n",
        "            user_embedding,\n",
        "            movie_embedding,\n",
        "            temporal_features,\n",
        "            genre_features\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.nn(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6MK3Ymmk0yi",
        "outputId": "55fca4e8-009c-4eef-e6fc-4d6e0b83f667"
      },
      "outputs": [],
      "source": [
        "num_epochs = 128\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "print(\"Is running on GPU:\", cuda)\n",
        "\n",
        "model = Recommender(n_users, n_items, n_factors=50, n_genres=20)\n",
        "print(model)\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n",
        "# GPU enable if you have a GPU...\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "# MSE loss\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# ADAM optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Prepare dataset\n",
        "train_set = MovieDataset(ratings_df, movies_df, train=True)\n",
        "val_set = MovieDataset(ratings_df, movies_df, train=False)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzJulaqYh4sY"
      },
      "source": [
        "## Training the Model\n",
        "We define a `train_model` function which trains the `Recommender` model and evaluates its performance on validation data. The training process includes the option of learning rate scheduling and saving the best-performing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ibF0l5qgh4sY"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=50, lr=0.001):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for features, targets in train_loader:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, targets in val_loader:\n",
        "                features, targets = features.to(device), targets.to(device)\n",
        "                outputs = model(features)\n",
        "                val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        print(f'Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fIDIdStk000",
        "outputId": "8377606a-7908-4e48-ff7e-fc0c1fa6ce5d"
      },
      "outputs": [],
      "source": [
        "train_model(model, train_loader, val_loader, epochs=num_epochs, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs_GVbSYmhib",
        "outputId": "47911410-f3f4-4785-ab50-875e664688a7"
      },
      "outputs": [],
      "source": [
        "# By training the model, we will have tuned latent factors for movies and users.\n",
        "c = 0\n",
        "uw = 0\n",
        "iw = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n",
        "        if c == 0:\n",
        "          uw = param.data\n",
        "          c +=1\n",
        "        else:\n",
        "          iw = param.data\n",
        "        #print('param_data', param_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IJgie01_p8k5"
      },
      "outputs": [],
      "source": [
        "trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-_tHZ_E_rub",
        "outputId": "ca61f438-c0d8-44aa-ee7c-5137b2618a95"
      },
      "outputs": [],
      "source": [
        "len(trained_movie_embeddings) # unique movie factor weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bl9s4iqXy75q"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# Fit the clusters based on the movie weights\n",
        "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MDzp-9u-m5n",
        "outputId": "ee885f8c-a6ac-40c7-fdfc-32be3304d0d3"
      },
      "outputs": [],
      "source": [
        "'''It can be seen here that the movies that are in the same cluster tend to have\n",
        "similar genres. Also note that the algorithm is unfamiliar with the movie name\n",
        "and only obtained the relationships by looking at the numbers representing how\n",
        "users have responded to the movie selections.'''\n",
        "for cluster in range(10):\n",
        "    print(\"Cluster #{}\".format(cluster))\n",
        "    movs = []\n",
        "    # Find movie indices belonging to the current cluster\n",
        "    for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
        "        movid = train_set.idx2movieid[movidx]\n",
        "        # Check how many ratings this movie has\n",
        "        rat_count = len(ratings_df.loc[ratings_df['movieId'] == movid])\n",
        "        movs.append((movie_names[movid], rat_count))\n",
        "    # Sort movies by rating count in descending order and print top 10\n",
        "    for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
        "        print(\"\\t\", mov[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
